<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>GRPO From Scratch | Ming  Yin</title>
    <meta name="author" content="Ming  Yin">
    <meta name="description" content="This post explains my pytorch implementation of Group Relative Policy Optimization Algorithm.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/Princeton_seal.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://mingyin0312.github.io/blog/2025/grpo/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "GRPO From Scratch",
      "description": "This post explains my pytorch implementation of Group Relative Policy Optimization Algorithm.",
      "published": "August 19, 2025",
      "authors": [
        {
          "author": "Ming Yin",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Princeton ECE",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">MingÂ </span>Yin</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">Research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching/Talks</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Miscellaneous</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>GRPO From Scratch</h1>
        <p>This post explains my pytorch implementation of Group Relative Policy Optimization Algorithm.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p><strong>Group Relative Policy Optimization</strong> <d-cite key="shao2024deepseekmath"></d-cite> is an RL algorithm that improves the modelâ€™s reasoning capability. It is a variant of <strong>Proximal Policy Optimization</strong> <d-cite key="schulman2017proximal"></d-cite> that is originally designed for robotic locomotion control and Atari game playing. GRPO turned out to be effective for rule-based reward, as demonstrated in DeepSeek-R1 <d-cite key="guo2025deepseek"></d-cite>.</p>

<p>To uncover the implementation details in the minimal way, I implemented GRPO from scratch with PyTorch in ðŸ‘‰ <a href="https://github.com/mingyin0312/RLFromScratch/blob/main/grpo_train_from_scratch.py" rel="external nofollow noopener" target="_blank">RLFromScratch</a>. Letâ€™s now understand it step by step.</p>

<hr>

<h3 id="quick-recap-of-grpo-algorithm">Quick Recap of GRPO Algorithm</h3>

<p>The objective function of GRPO has the following</p>

\[\mathcal{J}_{\mathrm{GRPO}}(\theta) 
= \mathbb{E}_{q,\,\{o_i\}_{i=1}^G \sim \pi_{\theta_{\mathrm{old}}}(\cdot \mid q)}
\Bigg[
\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}
\Big\{
\min \Big[
\Gamma_{i,t}\,\hat{A}_{i,t},\;
\mathrm{clip}\!\big(\Gamma_{i,t},\,1-\varepsilon,\,1+\varepsilon\big)\,\hat{A}_{i,t}
\Big]
- \beta\,\mathbb{D}_{\mathrm{KL}}\!\big[\pi_\theta \,\|\, \pi_{\mathrm{ref}}\big]
\Big\}
\Bigg] .\]

<p>where \(\Gamma_{i,t}:=\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}\), \(\varepsilon, \beta\) are hyper-parameters, and \(\hat{A}_{i,t}\) is the advantage calculated based on relative rewards of the outputs inside each group only with \(\hat{A}_{i, t}=\widetilde{r}_i=\frac{r_i-\operatorname{mean}(\mathbf{r})}{\operatorname{std}(\mathbf{r})}\). The unbiased estimator of KL divergence is \(\mathbb{D}_{K L}\left[\pi_\theta \| \pi_{r e f}\right]=\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-1\).</p>

<hr>

<h3 id="code-explanation">Code Explanation</h3>

<p><strong>- Reward Design</strong></p>

<p>I want to enforce the model to learn both the correct format and the correct answer. Format score is 1.0 and answer score is 2.0. The full reward is 3.0. The partial reward is 1.0.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_format_score</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Reward function that checks if the completion has the correct format.</span><span class="sh">"""</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">^&lt;reasoning&gt;(?:(?!&lt;/reasoning&gt;).)*&lt;/reasoning&gt;\n&lt;answer&gt;(?:(?!&lt;/answer&gt;).)*&lt;/answer&gt;$</span><span class="sh">"</span>
    <span class="n">matches</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bool</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="nf">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">g_a</span><span class="p">))</span> <span class="k">for</span> <span class="n">g_a</span> <span class="ow">in</span> <span class="n">batch_responses</span><span class="p">]</span>
    <span class="n">format_scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">match</span> <span class="k">else</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="k">match</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">format_scores</span>

<span class="k">def</span> <span class="nf">compute_reward</span><span class="p">(</span><span class="n">batch_answers</span><span class="p">,</span> <span class="n">answers</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Reward function that checks if the answer is correct.</span><span class="sh">"""</span>
    <span class="n">reward_scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span> <span class="k">if</span> <span class="n">g_a</span> <span class="o">==</span> <span class="n">a</span> <span class="k">else</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">g_a</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">batch_answers</span><span class="p">,</span> <span class="n">answers</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">reward_scores</span>
</code></pre></div></div>

<p><strong>- Initial Training Loop</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_sampler</span><span class="p">.</span><span class="nf">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">answers</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)):</span>
        <span class="n">prompt_enc</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">prompts</span><span class="p">,</span>
            <span class="n">return_tensors</span> <span class="o">=</span> <span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
            <span class="n">padding_side</span> <span class="o">=</span> <span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">prompt_enc</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># (B, prompt_len) and left_pad
</span>        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">prompt_enc</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_mask</span><span class="sh">"</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">input_ids</code> and <code class="language-plaintext highlighter-rouge">attention_mask</code> are used as inputs to generate responses (online exploration in RL).</p>

<p><strong>- Generate K Samples Per Prompt</strong></p>

<p>The number of Group is K.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">policy_model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">):</span>
        <span class="n">explore_generations</span> <span class="o">=</span> <span class="n">policy_model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">K</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span>
            <span class="p">)</span> <span class="c1"># (batch_size * K, prompt_len + max_new_tokens)
</span><span class="n">policy_model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="n">prompt_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">explore_generations</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span> 
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">explore_generations</code> and <code class="language-plaintext highlighter-rouge">batch_attention_mask</code> will be used to compute logits.</p>

<p><strong>- Compute logprobs</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">policy_model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">):</span>
        <span class="n">out_old</span> <span class="o">=</span> <span class="nf">policy_model</span><span class="p">(</span><span class="n">explore_generations</span><span class="p">,</span>  <span class="n">batch_attention_mask</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">logits_old</span> <span class="o">=</span> <span class="n">out_old</span><span class="p">.</span><span class="n">logits</span> <span class="c1"># [batch_size * K, seq_len, vocab_size]
</span><span class="n">policy_model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="c1"># shift logits and labels
</span><span class="n">logits_old</span> <span class="o">=</span> <span class="n">logits_old</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="nf">contiguous</span><span class="p">()</span> <span class="c1"># [batch_size * K, seq_len - 1, vocab_size]
</span><span class="n">labels_old</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:].</span><span class="nf">contiguous</span><span class="p">()</span> <span class="c1"># [batch_size * K, seq_len - 1]
</span><span class="n">logprobs_old</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span>
    <span class="n">logits_old</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits_old</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">labels_old</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">reduction</span> <span class="o">=</span> <span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span>
<span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">logits_old</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size * K, seq_len - 1]
</span><span class="k">assert</span> <span class="n">batch_action_mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">logprobs_old</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">batch_action_mask</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">batch_attention_mask</span><span class="p">.</span><span class="n">shape</span>
<span class="n">logprobs_old</span> <span class="o">=</span> <span class="n">logprobs_old</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span></code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">logprobs_old</code> denotes elementwise \(\log \pi_{\theta_{old}}(o_t\mid o_{&lt;t}, q)\). Similarly, <code class="language-plaintext highlighter-rouge">logprobs_ref</code> computes \(\log \pi_{\theta_{ref}}(o_t\mid o_{&lt;t}, q)\), and <code class="language-plaintext highlighter-rouge">logprobs_new</code> denotes \(\log \pi_{\theta}(o_t\mid o_{&lt;t}, q)\).</p>

<p><strong>- Compute advantages</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_responses_ids</span> <span class="o">=</span> <span class="n">explore_generations</span><span class="p">[:,</span> <span class="n">prompt_len</span><span class="p">:]</span> <span class="c1"># (batch_size*K, response_length) right pad
</span><span class="n">batch_responses</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">batch_decode</span><span class="p">(</span><span class="n">batch_responses_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="c1"># (batch_size*K, response_text_length)
</span><span class="n">batch_answers</span> <span class="o">=</span> <span class="p">[</span><span class="nf">extract_xml_answer</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">))]</span> <span class="c1"># (batch_size*K, generated_answer_length) str
</span><span class="n">answers_K</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">answers</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span>
<span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">batch_answers</span><span class="p">)</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">answers_K</span><span class="p">)</span>
<span class="n">batch_format_scores</span> <span class="o">=</span> <span class="nf">compute_format_score</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">)</span> <span class="c1"># (batch_size*K, 1)
</span><span class="n">batch_reward_scores</span> <span class="o">=</span> <span class="nf">compute_reward</span><span class="p">(</span><span class="n">batch_answers</span><span class="p">,</span> <span class="n">answers_K</span><span class="p">)</span> <span class="c1"># (batch_size*K, 1)
</span><span class="n">batch_rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">bfs</span> <span class="o">+</span> <span class="n">brs</span> <span class="k">for</span> <span class="n">bfs</span><span class="p">,</span> <span class="n">brs</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">batch_format_scores</span><span class="p">,</span> <span class="n">batch_reward_scores</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">batch_rewards</span> <span class="o">=</span> <span class="n">batch_rewards</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="c1"># (batch_size, K)
</span><span class="n">batch_advantages</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_rewards</span> <span class="o">-</span> <span class="n">batch_rewards</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_rewards</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="bp">True</span><span class="p">).</span><span class="nf">clamp_min</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">batch_advantages</span> <span class="o">=</span> <span class="n">batch_advantages</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span> <span class="c1"># (batch_size, K)
</span><span class="k">assert</span> <span class="n">batch_advantages</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">batch_advantages</span> <span class="o">=</span> <span class="n">batch_advantages</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">expand_as</span><span class="p">(</span><span class="n">logprobs_ref</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span><span class="k">assert</span> <span class="n">batch_advantages</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">logprobs_ref</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<p>The resulting <code class="language-plaintext highlighter-rouge">batch_advantages</code> has shape <code class="language-plaintext highlighter-rouge">[batch_size, K, seq_len - 1]</code>. The last dimension is simply a replication since \(\hat{A}_{i, t}=\widetilde{r}_i=\frac{r_i-\operatorname{mean}(\mathbf{r})}{\operatorname{std}(\mathbf{r})}\) is independent of sequence length t.</p>

<p><strong>- Valid mask</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">valid_mask</span> <span class="o">=</span> <span class="n">batch_action_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1] 
</span></code></pre></div></div>

<p>This is critical for the summation \(\frac{1}{\mid o_i\mid}\sum_{t=1}^{\mid o_i \mid}\) since every responses have vary lengths. Using <code class="language-plaintext highlighter-rouge">.mean()</code> will also take the invalid tokens into account due to padding, which is not what we want.</p>

<p><strong>- Compute probability ratios</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logprobs_new</span> <span class="o">-</span> <span class="n">logprobs_old</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span><span class="n">ratio_clipped</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">ppo_clip_range</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">ppo_clip_range</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span><span class="n">individual_ppo_reward</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="n">batch_advantages</span><span class="p">,</span> <span class="n">ratio_clipped</span> <span class="o">*</span> <span class="n">batch_advantages</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">ratio</code> computes pointwise \(\Gamma_{i,t}:=\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}\), <code class="language-plaintext highlighter-rouge">ratio_clipped</code> computes pointwise \(\mathrm{clip}\!\big(\Gamma_{i,t},\,1-\varepsilon,\,1+\varepsilon\big)\), and <code class="language-plaintext highlighter-rouge">individual_ppo_reward</code> denotes \(\min[
\Gamma_{i,t}\,\hat{A}_{i,t},\;
\mathrm{clip}\!\big(\Gamma_{i,t},\,1-\varepsilon,\,1+\varepsilon\big)\,\hat{A}_{i,t}]\).</p>

<p><strong>- Compute KL penalty</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ratio_ref_log</span> <span class="o">=</span> <span class="n">logprobs_ref</span> <span class="o">-</span> <span class="n">logprobs_new</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span><span class="n">ratio_ref</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ratio_ref_log</span><span class="p">)</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span><span class="n">individual_kl_penality</span> <span class="o">=</span> <span class="n">ratio_ref</span> <span class="o">-</span> <span class="n">ratio_ref_log</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># [batch_size, K, seq_len - 1]
</span></code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">individual_kl_penality</code> denotes \(\frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-\log \frac{\pi_{r e f}\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t}\right)}-1\).</p>

<p><strong>- Compute the overall GRPO loss</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum_loss_ave_response</span> <span class="o">=</span> <span class="p">(</span><span class="n">individual_ppo_reward</span> <span class="o">-</span> <span class="n">kl_coef</span> <span class="o">*</span> <span class="n">individual_kl_penality</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, K]
</span><span class="n">count_ave_response</span> <span class="o">=</span> <span class="n">valid_mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, K]
</span><span class="n">reward_ave_response</span> <span class="o">=</span> <span class="n">sum_loss_ave_response</span> <span class="o">/</span> <span class="n">count_ave_response</span> <span class="c1"># [batch_size, K]
</span><span class="n">grpo_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">reward_ave_response</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">sum_loss_ave_response</code> preformed operation $\sum_{t=1}^{\mid o_i \mid}$, and <code class="language-plaintext highlighter-rouge">reward_ave_response</code> preformed operation \(\frac{1}{\mid o_i \mid}\).</p>

<p>This concludes the key steps of GRPO.</p>

<!-- 

****

****
 -->

<h2 id="miscellaneous">Miscellaneous</h2>

<p>Also see this on twitter:</p>
<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet" data-width="500">
<p lang="en" dir="ltr">I implemented GRPO and DPO from scratch in vanilla Pytorch to unravel every piece of training details. Hope it could be helpful for those who care about the implementation details of the algorithms. ðŸ‘‰ <a href="https://t.co/1Exq7GTkLY" rel="external nofollow noopener" target="_blank">https://t.co/1Exq7GTkLY</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#AI</a> <a href="https://twitter.com/hashtag/RL?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#RL</a> <a href="https://twitter.com/hashtag/LLM?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#LLM</a></p>â€” Ming Yin (@MingYin_0312) <a href="https://twitter.com/MingYin_0312/status/1955351703626154017?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">August 12, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/grpo.bib"></d-bibliography>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2025 Ming  Yin. 
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9NJFB4PFB4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-9NJFB4PFB4');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
