<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>On Reinforcement Learning for Large Language Models | Ming  Yin</title>
    <meta name="author" content="Ming  Yin">
    <meta name="description" content="A personal thinking on why reinforcement learning is vital for Large Language Models. [Updated 02/21]">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/Princeton_seal.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://mingyin0312.github.io/blog/2025/rl-llm/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "On Reinforcement Learning for Large Language Models",
      "description": "A personal thinking on why reinforcement learning is vital for Large Language Models. [Updated 02/21]",
      "published": "January 7, 2025",
      "authors": [
        {
          "author": "Ming Yin",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Princeton, ECE",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ming </span>Yin</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">Research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching/Talks</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Miscellaneous</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>On Reinforcement Learning for Large Language Models</h1>
        <p>A personal thinking on why reinforcement learning is vital for Large Language Models. [Updated 02/21]</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p>Reinforcement Learning seems to be the right learning paradigm for AI (if we believe AI should think like us). Why? If we have a goal, we take action and get feedback, and then we learn from the feedback until we achieve the goal.</p>

<h3 id="rl-with-reward-signal">RL with Reward Signal</h3>

<p>Arguably, the most convincing RL method so far is Alpha-Go/Zero <d-cite key=" silver2016mastering,silver2017mastering"></d-cite>. In July 2024, Google-Deepmind presented AlphaProof, a new reinforcement-learning-based system for formal math reasoning. When tackling this year’s International Mathematical Olympiad, it achieved the same level as a silver medalist in the competition.</p>

<p>It utilizes models to generate proofs, uses reward oracle (formal language Lean) to generate feedback (right or wrong), and uses RL algorithms to optimize. It becomes successful by scaling up the model. Of course, there are many other components, such as the Formalizer network, but, as mentioned in <a href="https://www.youtube.com/watch?v=pkpJMNjvgXw" rel="external nofollow noopener" target="_blank">David’s talk</a>, <strong>it is essentially just RL at a large scale.</strong></p>

<p>Another example is the ongoing <a href="https://www.youtube.com/watch?v=yCIYS9fx56U" rel="external nofollow noopener" target="_blank">Reinforcement Fine-Tunning</a> (RFT)  from OpenAI. This program allows developers to fine-tune existing models by providing a small set (dozens to thousands) of high-quality questions and reference answers. It says, “This technique reinforces how the model reasons through similar problems and improves its accuracy on specific tasks in that domain.” This is quite exciting, as we can use only a small number of samples (I think 10-1000 is small nowadays) to achieve amazing performance in advanced scientific tasks via RL algorithms.</p>

<p>More importantly, these are truly what humans do when facing a new task: We take action and get feedback, then “learn from the feedback” until we achieve our goal. This makes the idea of RL so natural for learning and improving the LLMs/AI. And then, different “learn from the feedback” will yield different RL algorithms.</p>

<p>Good RL algorithms for LLMs are akin to good learning habits in humans.</p>

<h3 id="offline-rl-vs-imitation-learning">Offline RL vs. Imitation Learning</h3>

<p>RL has yet to exhibit its full potential for LLMs. Existing post-training/fine-tunning methods are mostly “Imitation Learning” style. For example, the popular Direct Preference Optimization algorithm uses the Bradley-Terry model to construct the negative log-likelihood loss over the preference data—essentially an imitation learning approach.</p>

<p>What offline RL should do is stitch, effectively ‘stitching’ together (and also modifying) parts of the historical demonstrations to generate unseen, higher-quality ones. This idea is reminiscent of how we write papers—We have all the existing papers and leverage existing techniques together to generate new techniques/results.</p>

<p>It is natural that the same thing should happen to LLMs, which is offline RL (Of course, we should not go too far to forget the safety concerns, so adding a KL regularizer :)). There are some efforts in the simulated robotics tasks <d-cite key=" hepburn2022model,zhou2024free"></d-cite>, but very few for LLMs (if any).</p>

<h3 id="rl-without-feedback">RL without Feedback</h3>

<p>In many situations, having feedback is not possible, and the RL loop contains only state and action. In this case, humans can still learn from previous responses/actions (historical data), reasoning about them, and coming up with an improved solution. It might be critical that LLMs also consider this perspective.</p>

<h3 id="a-bright-future-for-rl">A Bright Future for RL</h3>

<p>While RL has its limitations and challenges, its potential for advancing AI is immense. From improving LLMs to tackling complex problems like formal reasoning and beyond, RL offers a natural, human-like learning paradigm. Let’s focus on the bright side as we continue to explore its possibilities!</p>

<hr>

<h3 id="0221-update-rl-significantly-improves-math-reasoning-on-the-gsm8k-data">02/21 Update: RL significantly improves Math Reasoning on the GSM8K Data</h3>

<p><a href="https://api-docs.deepseek.com/news/news250120" rel="external nofollow noopener" target="_blank">DeepSeek-R1(-Zero)</a> exhibits surprising capability for reasoning tasks such as Math and Coding <d-cite key=" guo2025deepseek"></d-cite>, rivaling openai-o1.</p>

<p>I also tried it myself, and here is my Setup:</p>

<ul>
  <li>
    <p>Base Model: <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" rel="external nofollow noopener" target="_blank">Qwen2.5-1.5B-Instruct</a>;</p>
  </li>
  <li>
    <p>Dataset: <a href="https://huggingface.co/datasets/openai/gsm8k" rel="external nofollow noopener" target="_blank">GSM8K</a>; Training size: 7472 , Testing size: 1319.</p>
  </li>
</ul>

<p>RL Training:</p>

<ul>
  <li>
    <p>Group Relative Policy Optimization (GRPO) <d-cite key="shao2024deepseekmath"></d-cite>, <code class="language-plaintext highlighter-rouge">GRPOConfig, GRPOTrainer</code> from <code class="language-plaintext highlighter-rouge">trl.trainer</code>;</p>
  </li>
  <li>
    <p>Epoch: 1, Learning Rate: 1e-5;</p>
  </li>
  <li>
    <p>8 H100s training for 5Hrs.</p>
  </li>
</ul>

<p>SFT Training:</p>

<ul>
  <li>
    <p>Supervised Fine-tuning, <code class="language-plaintext highlighter-rouge">SFTTrainer, SFTConfig</code> from <code class="language-plaintext highlighter-rouge">trl</code>;</p>
  </li>
  <li>
    <p>Epoch: 2, Learning Rate: 1e-6; The (Epoch, Learning Rate) pair is determined after hyperparameter search, This setup gives the highest accuracy on the test set;</p>
  </li>
  <li>
    <p>4 H100s training for 10Mins.</p>
  </li>
</ul>

<p>Evaluation:</p>

<ul>
  <li>Correct if <code class="language-plaintext highlighter-rouge">generated_answer == true_answer</code>.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/RLLLM.png" style="width: 65%; height: auto;">
    </div>
</div>

<p><br><br></p>

<p><strong>Takeaway:</strong> Reinforcement Learning significantly enhances the original model’s performance by 27% and surpasses the SFT approach by nearly 20%. This demonstrates RL’s strong potential for tackling challenging reasoning tasks! However, this improvement comes at the cost of increased computational overhead. The code is available <a href="https://github.com/mingyin0312/RL4LLM?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">here</a>.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/rl-llm.bib"></d-bibliography>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Ming  Yin. 
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9NJFB4PFB4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-9NJFB4PFB4');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
